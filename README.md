# **NLP**
![Banner del Proyecto](banner.jpg)

Este proyecto tiene como objetivo mostrar la resoluci√≥n de los desaf√≠os evaluatorios de la materia Procesamiento de Lenguaje Natural, del Posgrado Carrera de Especializaci√≥n en Inteligencia Artificial, del Laboratorio de Sistemas Embebidos, de la Facultad de Ingenier√≠a de la Universidad de Buenos Aires, realizados durante el quinto bimestre de cursada del a√±o 2024.

---

## **Tabla de Contenidos**
1. [Descripci√≥n](#descripci√≥n)  
2. [Desaf√≠os](#desaf√≠os)
3. [Caracter√≠sticas](#caracter√≠sticas)  
4. [Instalaci√≥n](#instalaci√≥n)  
5. [Cr√©ditos](#cr√©ditos)  
6. [Alumno](#alumno) 
7. [Contacto](#contacto)   

---

## **Descripci√≥n**
A modo de evaluar lo aprendido en la materia, se plantearon cuatro desaf√≠os a resolver. Cada uno de ellos, permite utilizar los conocimientos adquiridos durante el desarrollo de a materia. Los temas a desarrollar, fueron los siguientes:

1. Vectorizaci√≥n de documentos y Entrenamiento de modelos de clasificaci√≥n Na√Øve Bayes.
2. Vectores de Gensim y Test de Analog√≠as.
3. Generaci√≥n de Secuencias.
4. Construir un Question/Answer Bot.

---

## **Desaf√≠os**
1. Desaf√≠o N¬∫1:
- Utilizando el conjunto de datos 20 Newsgroups, se realizan tareas de procesamiento de lenguaje natural, incluyendo vectorizaci√≥n de texto con TfidfVectorizer y CountVectorizer, c√°lculo de similitudes coseno entre documentos, y clasificaci√≥n con modelos de Na√Øve Bayes (MultinomialNB y ComplementNB). Eval√∫a el desempe√±o mediante la m√©trica F1-score macro y optimiza par√°metros de vectorizaci√≥n (como n-gramas, eliminaci√≥n de palabras vac√≠as y tama√±o del vocabulario) y de los modelos. Adem√°s, se explora la similitud entre documentos y palabras utilizando matrices transpuestas (documento-t√©rmino a t√©rmino-documento) para comprender relaciones sem√°nticas.

2. Desaf√≠o N¬∫2:
- Entrena modelos Word2Vec utilizando un corpus de texto, transformando oraciones en secuencias de palabras, generando vectores de palabras con los enfoques Skip-Gram y CBOW. Eval√∫a relaciones sem√°nticas mediante analog√≠as y palabras similares, y visualiza embeddings en 2D y 3D usando reducci√≥n de dimensionalidad (TSNE). Tambi√©n exporta los vectores generados y etiquetas para su an√°lisis en herramientas externas. El c√≥digo incluye visualizaciones interactivas de relaciones entre palabras y m√©tricas como p√©rdida durante el entrenamiento.

3. Desaf√≠o N¬∫3:
- Esta serie de scripts implementan modelos de redes neuronales GRU, LSTM y RNN para generar texto basado en un corpus literario. Inicia procesando un archivo de texto dividi√©ndolo en oraciones y palabras, tokeniz√°ndolas y convirti√©ndolas en secuencias num√©ricas. Luego, divide los datos en conjuntos de entrenamiento y validaci√≥n, ajusta el tama√±o del contexto para secuencias, y genera sub-secuencias con padding. Se define un modelo con capas de embedding, LSTM, y softmax para predecir palabras. Se utiliza la m√©trica de perplejidad en un callback personalizado para monitorear el desempe√±o durante el entrenamiento y aplicar *early stopping*. Finalmente, implementa una interfaz con Gradio para que el modelo genere texto de forma interactiva, agregando palabras a una frase inicial dada por el usuario.

4. Desaf√≠o N¬∫4:
- Implementa un modelo secuencial basado en codificador-decodificador con PyTorch para realizar generaci√≥n de texto a partir de conversaciones. El flujo incluye la descarga de datos, limpieza y preprocesamiento del texto, construcci√≥n de vocabularios y conversi√≥n a secuencias de √≠ndices. Utiliza embeddings preentrenados (GloVe) para representar palabras. Define una arquitectura de red neuronal recurrente (LSTM) para el codificador y el decodificador, y entrena el modelo utilizando datos divididos en conjuntos de entrenamiento y validaci√≥n. Durante el entrenamiento, monitorea las m√©tricas de p√©rdida y precisi√≥n. Finalmente, permite realizar inferencias con un ajuste de aleatoriedad (temperature sampling) para generar respuestas m√°s variadas. Tambi√©n incluye funciones para guardar los modelos y graficar las m√©tricas.

---

## **Caracter√≠sticas**
- ‚úÖ F√°cil de usar.  
- üìñ Did√°ctico.  
- üíª‚Äã Aprendizaje puesto en pr√°ctica.  
- üèãüèΩ Escaso costo computacional.  

---

## **Instalaci√≥n**
Pasos para instalar y configurar el proyecto:  

### Requisitos Previos 
- ![Python](https://img.shields.io/badge/-Python-333333?style=flat&logo=python) 
- üõú Acceso a internet  

### Pasos
1. Clona el repositorio:  
   ```bash  
   git clone https://github.com/fabriciolopretto/NLP.git  
   cd NLP  

---

## **Cr√©ditos:**
Agradecimientos a las siguientes bibliotecas y recursos:

- Gensim
- Keras
- Matplotlib
- Numpy
- Pandas
- Plotly
- Pytorch
- Seaborn
- Sklearn
- Tensorflow

---

## **Docente:**
:octocat: Dr. Rodrigo Cardenas Szigety

---

## **Alumno:**
Fabricio Lopretto (a1616) <img src="https://raw.githubusercontent.com/iampavangandhi/iampavangandhi/master/gifs/Hi.gif" width="30px"></h1>.

---

## **Contacto:**
Para dudas o sugerencias, contacto en:
<a href="fabriciolopretto@gmail.com.ar"><img alt="Email" src="https://img.shields.io/badge/Gmail-Maurovera069@gmail.com-blue?style=flat-square&logo=gmail"></a> 
<a href="https://www.linkedin.com/in/fabricio-lopretto-scientific-analyst/"><img alt="LinkedIn" src="https://img.shields.io/badge/LinkedIn-Mauro%20Vera-blue?style=flat-square&logo=linkedin"></a>
